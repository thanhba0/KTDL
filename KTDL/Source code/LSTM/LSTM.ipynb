{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LSTM.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"AN8rgbmrdKJP","colab":{}},"source":["# -*- coding: utf-8 -*-\n","%tensorflow_version 1.x\n","!pip install keras==2.2.5\n","!pip install pyvi\n","\n","import numpy as np\n","from numpy import random\n","import os, pickle, re, keras, sklearn, string\n","from keras.callbacks import *\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from pyvi import ViTokenizer, ViPosTagger\n","from keras.layers import *\n","from keras.models import Model\n","from keras import optimizers\n","import gensim, operator, json\n","import pandas as pd\n","from sklearn.metrics import *\n","import keras.backend as K\n","from keras.models import *\n","from keras import initializers, regularizers\n","from keras import optimizers\n","from keras.engine.topology import Layer\n","from keras import constraints\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rp1-Qa_81FEY","colab_type":"code","colab":{}},"source":["# Đọc đường dẫn data lưu trên google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","path_train ='/content/gdrive/My Drive/Project/Emotion Text Classification/data/train_nor_811.csv'\n","path_valid ='/content/gdrive/My Drive/Project/Emotion Text Classification/data/valid_nor_811.csv'\n","path_test ='/content/gdrive/My Drive/Project/Emotion Text Classification/data/test_nor_811.csv'\n","path_stopword = '/content/gdrive/My Drive/Emotion Text Classification/data/stopwords.txt'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Skg7lUZ9LYAn","colab":{}},"source":["# Hàm đọc thư viện pretrain word embedding.\n","# Bộ pretrain word embedding được huấn luyện trên báo mới, news có 300 chiều\n","\n","path_embedding= '/content/gdrive/My Drive/Project/Emotion Text Classification/embedding/baomoi.vn.model.bin'\n","\n","import io\n","from gensim.models import KeyedVectors\n","\n","word_embedding = KeyedVectors.load_word2vec_format(path_embedding, binary=True)\n","\n","# Ví dụ lấy vector của 1 từ trong pretrain word embedding\n","EMBEDDING_DIM = word_embedding['yêu'].shape[0]\n","print(\"Chieu du lieu embedding: \",EMBEDDING_DIM)\n","# Vector của từ yêu trong bộ pretrained word embedding.\n","print(word_embedding['yêu'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L1_TFsm9WdA0","colab":{}},"source":["# Các hàm tiền xử lý \n","def tokenizer(text):\n","    token = ViTokenizer.tokenize(text)\n","    return token\n","\n","# Các bước tiền xử lý dữ liệu tiếng Việt\n","# Xóa các icon trong văn bản\n","\n","def deleteIcon(text):\n","    text = text.lower()\n","    s = ''\n","    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n","    for char in text:\n","        if char !=' ':\n","            if len(re.findall(pattern, char)) != 0:\n","                s+=char\n","            elif char == '_':\n","                s+=char\n","        else:\n","            s+=char\n","    s = re.sub('\\\\s+',' ',s)\n","    return s.strip()\n","\n","# Hàm xử lý chính tiền xử lý\n","def clean_doc(doc):\n","    doc = tokenizer(doc)\n","    # xóa tất cả dấu câu (!,?..) trong câu\n","    for punc in string.punctuation:\n","        if punc != \"_\":\n","            doc = doc.replace(punc,' ')\n","    doc = deleteIcon(doc)\n","    \n","    # Xóa các số \n","    doc = re.sub(r\"[0-9]+\", \" num \", doc)\n","    # Đưa về chữ thường\n","    doc = doc.lower()\n","    # Xóa nhiều khoảng trắng\n","    doc = re.sub('\\\\s+',' ',doc)\n","    return doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Usy-4ygPWdA2","colab":{}},"source":["# hàm đọc dữ liệu trên 3 tập train dev test và chạy tiền xử lý \n","train_data = pd.read_csv(path_train)\n","valid_data = pd.read_csv(path_valid)\n","test_data = pd.read_csv(path_test)\n","\n","X_train = train_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n","y_train = train_data[\"Emotion\"]\n","\n","X_val = valid_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n","y_val = valid_data[\"Emotion\"]\n","\n","X_test = test_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n","y_test = test_data[\"Emotion\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnYcGTbL0jnu","colab_type":"code","colab":{}},"source":["# Hiển thị kết quả số lượng tập train và tập test\n","print(len(X_train),len(y_train))\n","print(len(X_val),len(y_val))\n","print(len(X_test),len(y_test))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fyIQeZi-WdA4","colab":{}},"source":["classes = ['Anger','Disgust','Enjoyment','Fear','Other','Sadness','Surprise']\n","def to_category_vector(label):\n","    vector = np.zeros(len(classes)).astype(np.float64)\n","    index = classes.index(label)\n","    vector[index] = 1.0\n","    return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fqYyTPmLWdA_","colab":{}},"source":["# Chuyển nhãn thành số trong tập train và tập test\n","\n","y_train_encode = []\n","for label in y_train:\n","    y_train_encode.append(to_category_vector(label))\n","\n","\n","y_val_encode = []\n","for label in y_val:\n","    y_val_encode.append(to_category_vector(label))\n","\n","print(classes)\n","print(y_train_encode[0])\n","print(y_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0EB_NYPlWdBC","colab":{}},"source":["# Tất cả các từ vựng trong tập X_train nó sẽ tạo thành từ điển\n","# mỗi vector của từ đầu vào, nó sẽ chuyển thành 1 vector với số chiều cố định và mỗi các từ vựng sẽ thay bằng chỉ số index của nó trong tập từ điển\n","# Số chiều vector mỗi đầu vào ta sẽ lấy câu dài nhất là chiều vector đâu vào và các caua ngắn hơn sẽ tự động thêm giá trị 0 phía sau\n","xLengths = [len(x.split(' ')) for x in X_train]\n","h = sorted(xLengths)  #sorted lengths\n","maxLength =h[len(h)-1]\n","print(\"Giá trị độ dài câu dài nhất: \",maxLength)\n","input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n","input_tokenizer.fit_on_texts(X_train)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","word_index = input_tokenizer.word_index\n","print(\"input_vocab_size:\",input_vocab_size)\n","X_train_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength,padding=\"post\"))\n","\n","# Ví dụ đầu vào mô hình LSTM\n","print(\"Câu đầu vào : \", X_train[0])\n","print(\"Câu đầu vào sau khi encode : \",X_train_encode[0])\n","\n","X_val_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_val), maxlen=maxLength,padding=\"post\"))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Td-ML6bqWdBE","colab":{}},"source":["# Hàm lấy vector của từ vựng trong pre-trained word embedding \n","def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n","    count6 = 0\n","    countNot6 = 0\n","    #embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) \n","    embedding_matrix = np.asarray([np.random.uniform(-0.01,0.01,EMBEDDING_DIM) for _ in range((len(word_index) + 1))])\n","    list_oov = []\n","    word_is_trained = []\n","    for word, i in word_index.items():\n","        try:\n","            embedding_vector = model_embedding[word]\n","            word_is_trained.append(word)\n","        except:\n","            continue\n","        if embedding_vector is not None:\n","            count6 +=1\n","            embedding_matrix[i] = embedding_vector\n","    \n","    print('Number of words in pre-train embedding: ' + str(count6))\n","    print('Number of words not in pre-train embedding: ' + str(countNot6))\n","    return embedding_matrix,word_is_trained"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HRIghLqMBuhV","colab":{}},"source":["embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n","print(word_is_trained)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oYeHeBL-wqO","colab_type":"code","colab":{}},"source":["\"\"\"\n","# ATTENTION LAYER\n","Cite these works \n","1. Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n","\"Hierarchical Attention Networks for Document Classification\"\n","accepted in NAACL 2016\n","\"\"\"\n","def dot_product(x, kernel):\n","\tif K.backend() == 'tensorflow':\n","\t\treturn K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","\telse:\n","\t\treturn K.dot(x, kernel)\n","\n","class AttentionWithContext(Layer):\n","\tdef __init__(self,\n","\t\t\t\t W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","\t\t\t\t W_constraint=None, u_constraint=None, b_constraint=None,\n","\t\t\t\t bias=True, **kwargs):\n","\n","\t\tself.supports_masking = True\n","\t\tself.init = initializers.get('glorot_uniform')\n","\n","\t\tself.W_regularizer = regularizers.get(W_regularizer)\n","\t\tself.u_regularizer = regularizers.get(u_regularizer)\n","\t\tself.b_regularizer = regularizers.get(b_regularizer)\n","\n","\t\tself.W_constraint = constraints.get(W_constraint)\n","\t\tself.u_constraint = constraints.get(u_constraint)\n","\t\tself.b_constraint = constraints.get(b_constraint)\n","\n","\t\tself.bias = bias\n","\t\tsuper(AttentionWithContext, self).__init__(**kwargs)\n","\n","\tdef build(self, input_shape):\n","\t\tassert len(input_shape) == 3\n","\n","\t\tself.W = self.add_weight((input_shape[-1], input_shape[-1],),\n","\t\t\t\t\t\t\t\t initializer=self.init,\n","\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n","\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n","\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n","\t\tif self.bias:\n","\t\t\tself.b = self.add_weight((input_shape[-1],),\n","\t\t\t\t\t\t\t\t\t initializer='zero',\n","\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n","\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n","\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n","\n","\t\tself.u = self.add_weight((input_shape[-1],),\n","\t\t\t\t\t\t\t\t initializer=self.init,\n","\t\t\t\t\t\t\t\t name='{}_u'.format(self.name),\n","\t\t\t\t\t\t\t\t regularizer=self.u_regularizer,\n","\t\t\t\t\t\t\t\t constraint=self.u_constraint)\n","\n","\t\tsuper(AttentionWithContext, self).build(input_shape)\n","\n","\tdef compute_mask(self, input, input_mask=None):\n","\t\t# do not pass the mask to the next layers\n","\t\treturn None\n","\n","\tdef call(self, x, mask=None):\n","\t\tuit = dot_product(x, self.W)\n","\n","\t\tif self.bias:\n","\t\t\tuit += self.b\n","\n","\t\tuit = K.tanh(uit)\n","\t\tait = dot_product(uit, self.u)\n","\n","\t\ta = K.exp(ait)\n","\n","\t\tif mask is not None:\n","\t\t\ta *= K.cast(mask, K.floatx())\n","\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","\t\ta = K.expand_dims(a)\n","\t\tweighted_input = x * a\n","\t\t\n","\t\treturn weighted_input\n","\n","\tdef compute_output_shape(self, input_shape):\n","\t\treturn input_shape[0], input_shape[1], input_shape[2]\n","\t\n","class Addition(Layer):\n","\tdef __init__(self, **kwargs):\n","\t\tsuper(Addition, self).__init__(**kwargs)\n","\n","\tdef build(self, input_shape):\n","\t\tself.output_dim = input_shape[-1]\n","\t\tsuper(Addition, self).build(input_shape)\n","\n","\tdef call(self, x):\n","\t\treturn K.sum(x, axis=1)\n","\n","\tdef compute_output_shape(self, input_shape):\n","\t\treturn (input_shape[0], self.output_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8b-4SQdfzFQ8","colab":{}},"source":["# Khởi tạo mô hình LSTM. \n","filter_nums = 256 # best 128\n","def build_model():\n","        inputs  = Input(shape=(maxLength, ), dtype='float64', name='inputs')    \n","        embedding_layer = Embedding(input_vocab_size,EMBEDDING_DIM,weights=[embedding_matrix], input_length=maxLength, trainable=True,name = 'word_emb')(inputs)\n","        embedding_layer = SpatialDropout1D(0.75)(embedding_layer)\n","                \n","              \n","        lstm_feature1 = CuDNNLSTM(filter_nums, return_sequences=True)(embedding_layer)\n","\n","        att1 = AttentionWithContext()(lstm_feature1)\n","        att1 = Addition()(att1)\n","\n","        fc1 = Dropout(0.5)(Dense(256, name = 'dense_1')(att1))\n","        output1 = Dense(len(classes),name=\"output1\", activation='softmax')(fc1)\n","\n","    \n","        # define optimizer\n","\n","        model = Model(inputs=inputs, outputs=output1)\n","        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n","        \n","        model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","        \n","        history = model.fit(X_train_encode, np.array(y_train_encode), validation_data = (X_val_encode,np.array(y_val_encode)) , batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n","        return model\n","\n","model = build_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cQwJ75p_WdBJ","colab":{}},"source":["X_test_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength,padding=\"post\"))\n","test_length = len(X_test_encode)\n","\n","y_predict = []\n","predicted = model.predict(X_test_encode)\n","for predict in predicted:\n","    index2, value = max(enumerate(predict), key=operator.itemgetter(1))\n","    y_predict.append(classes[index2])\n","    \n","print(y_predict[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"CWnvBNeI0joN","colab_type":"code","colab":{}},"source":["print(y_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoEymwgf0joR","colab_type":"code","colab":{}},"source":["# Dự đoán kết quả trên các độ đo\n","# precision = precision_score(y_test, y_predict, average='weighted')\n","# recall = recall_score(y_test, y_predict, average='weighted')\n","# f1score = f1_score(y_test, y_predict, average='micro')\n","accuracy = accuracy_score(y_test, y_predict)\n","\n","# print(\"Kết quả mô hình LSTM + Attention layer\")\n","print(\"Kết quả mô hình\")\n","# print(\"Precision: \", precision)\n","# print(\"Recall: \", recall)\n","# print(\"F1-Score: \", f1score)\n","print(\"Accuracy: \", accuracy)\n","\n","print(classification_report(y_test,y_predict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WyndWU_0joU","colab_type":"code","colab":{}},"source":["# Chương trình demo nhập vào 1 câu\n","\n","demo = \"Báo cáo thật là vui\"\n","# chạy hàm tiền xử lý\n","demo_pre = clean_doc(demo)\n","# Đưa vào mô hình dự đoán\n","X_demo_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences([demo_pre]), maxlen=maxLength,padding=\"post\"))\n","# Hiển thị kết quả encode câu đầu vào\n","# print(X_demo_encode)\n","predicted = model.predict(X_demo_encode)\n","index2, value = max(enumerate(predicted[0]), key=operator.itemgetter(1))\n","print(\"Kết quả dự đoán:\", classes[index2])"],"execution_count":null,"outputs":[]}]}